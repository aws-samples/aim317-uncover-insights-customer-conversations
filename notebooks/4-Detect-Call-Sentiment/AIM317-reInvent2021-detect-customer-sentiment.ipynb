{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae9a415a",
   "metadata": {},
   "source": [
    "# Detect sentiment in customer calls using Amazon Comprehend\n",
    "\n",
    "Intro to be written..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e380579",
   "metadata": {},
   "source": [
    "### Import libraries and initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095fab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "inprefix = 'comprehend/input'\n",
    "outprefix = 'quicksight/temp/insights'\n",
    "# Amazon Comprehend client\n",
    "comprehend = boto3.client('comprehend')\n",
    "# Amazon S3 clients\n",
    "s3 = boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "bucket = '<enter-bucket-name-here>' \n",
    "assert bucket != '<enter-bucket-name-here>', \"Bucket name not found. Copy the S3 bucket name from CloudFormation stack and re-run the cell.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc94e1",
   "metadata": {},
   "source": [
    "### Detect sentiment of transcripts\n",
    "Write about Comprehend detect sentiments here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to page through our transcripts in S3\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "pages = paginator.paginate(Bucket=bucket, Prefix=inprefix)\n",
    "job_name_list = []\n",
    "t_prefix = 'quicksight/data/sentiment'\n",
    "\n",
    "# We will define a DataFrame to store the results of the sentiment analysis\n",
    "cols = ['transcript_name', 'sentiment']\n",
    "df_sent = pd.DataFrame(columns=cols)\n",
    "\n",
    "# Now lets page through the transcripts\n",
    "for page in pages:\n",
    "    for obj in page['Contents']:\n",
    "        # get the transcript file name\n",
    "        transcript_file_name = obj['Key'].split('/')[2]\n",
    "        # now lets get the transcript file contents\n",
    "        temp = s3_resource.Object(bucket, obj['Key'])\n",
    "        transcript_contents = temp.get()['Body'].read().decode('utf-8')\n",
    "        # Call Comprehend to detect sentiment\n",
    "        response = comprehend.detect_sentiment(Text=transcript_contents, LanguageCode='en')\n",
    "        # Update the results DataFrame with the cta predicted label\n",
    "        # Create a CSV file with cta label from this DataFrame\n",
    "        df_sent.loc[len(df_sent.index)] = [transcript_file_name.strip('en-').strip('.txt'),response['Sentiment']]\n",
    "        \n",
    "df_sent.to_csv('s3://' + bucket + '/' + t_prefix + '/' + 'sentiment.csv', index=False)\n",
    "df_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1befc863",
   "metadata": {},
   "source": [
    "## End of notebook. Please go back to the workshop instructions to review the next steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
